{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04 Long Short-Term Memory (LSTM).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GBlIOcB6yht"
      },
      "source": [
        "# Long Short-Term Memory (LSTM)\n",
        "\n",
        "* Import\n",
        "* Custom Dataset\n",
        "* Dataset DataLoader\n",
        "* ** LSTM Model **\n",
        "* Loss Function & Optimizer\n",
        "* Training Model\n",
        "* Testing \n",
        "\n",
        "**Reference**\n",
        "\n",
        "* LSTM Pytorch [Doc](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nRik_CG2Wsw"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgsp44qr2GMI"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EgYjKWH2cVU"
      },
      "source": [
        "## Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2rZcwSr2YH1"
      },
      "source": [
        "class StockDataset(Dataset):\n",
        "    def __init__(self, seq_length, file_path, train=True, transform=None):\n",
        "        \n",
        "        # read file\n",
        "        df = pd.read_excel(file_path)\n",
        "        df_len = len(df) - 1\n",
        "\n",
        "        df_len -= ( df_len % seq_length )\n",
        "        total_size = df_len // seq_length\n",
        "        train_size = int(total_size * 0.7)\n",
        "        \n",
        "        f = df.iloc[:(train_size*seq_length), 3:7] # data\n",
        "        l = df.iloc[1:(train_size*seq_length)+1, 7:] #label\n",
        "        data_size = train_size\n",
        "        \n",
        "        if not train:\n",
        "            f = df.iloc[(train_size*seq_length):df_len, 3:7] # data\n",
        "            l = df.iloc[(train_size*seq_length)+1:df_len+1, 7:] #label\n",
        "            data_size = total_size - train_size\n",
        "        \n",
        "        self.features = np.array(f).reshape(data_size, seq_length, 4)\n",
        "        self.labels = np.array(l).reshape(data_size, seq_length, 1)\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        features = self.features[index]\n",
        "        labels = self.labels[index]\n",
        "        \n",
        "        if self.transform:\n",
        "            features = self.transform(features)\n",
        "            labels = self.transform(labels)\n",
        "\n",
        "        return (features, labels)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7JKoPu72hkO"
      },
      "source": [
        "## Dataset & Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i66TtNCv2ruh"
      },
      "source": [
        "# Transform function\n",
        "def toTensor(x):\n",
        "  return torch.tensor(x)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHBiWluy2jsw"
      },
      "source": [
        "# dataset\n",
        "train_dataset = StockDataset(\n",
        "    file_path=r'Stock.xlsx',\n",
        "    seq_length=5,\n",
        "    train=True,\n",
        "    transform=toTensor\n",
        "    )\n",
        "\n",
        "test_dataset = StockDataset(\n",
        "    file_path = 'Stock.xlsx',\n",
        "    seq_length = 5,\n",
        "    train = False,\n",
        "    transform = toTensor\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyWeS5Ir8O4S",
        "outputId": "1fb3bc44-1070-4752-99ba-aad329f1dd4e"
      },
      "source": [
        "fetures, labels = train_dataset[0:7]\n",
        "fetures.shape, labels.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([7, 5, 4]), torch.Size([7, 5, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y6jWNNW3Ak2"
      },
      "source": [
        "# dataloader\n",
        "train_loader = DataLoader(\n",
        "    dataset = train_dataset, \n",
        "    batch_size = 4,\n",
        "    shuffle = True\n",
        "    )\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset = test_dataset, \n",
        "    batch_size = 4,\n",
        "    shuffle = False\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwVxcatG8TkI",
        "outputId": "d8042163-cd3a-4167-af03-6475cba952f8"
      },
      "source": [
        "for idx, (seqs, labels) in enumerate(train_loader):\n",
        "    print(seqs.shape)\n",
        "    print(seqs)\n",
        "    break"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 5, 4])\n",
            "tensor([[[3272.1900, 3286.6700, 3265.7600, 3281.0000],\n",
            "         [3280.6200, 3295.1900, 3275.0500, 3286.6200],\n",
            "         [3285.4600, 3285.4600, 3265.0100, 3269.3900],\n",
            "         [3266.2200, 3290.3900, 3244.4000, 3288.9700],\n",
            "         [3283.8400, 3284.9300, 3262.2800, 3273.8300]],\n",
            "\n",
            "        [[3148.0200, 3150.4600, 3134.6100, 3140.0100],\n",
            "         [3148.9900, 3157.0300, 3132.6200, 3156.2100],\n",
            "         [3152.2400, 3186.9800, 3146.6400, 3147.4500],\n",
            "         [3138.4400, 3158.0500, 3118.0900, 3157.8700],\n",
            "         [3157.0000, 3187.8900, 3156.9800, 3185.4400]],\n",
            "\n",
            "        [[3242.2200, 3254.3400, 3239.8800, 3253.3300],\n",
            "         [3252.6900, 3261.3800, 3243.8400, 3261.2200],\n",
            "         [3258.8300, 3264.0800, 3236.3500, 3251.3800],\n",
            "         [3246.8600, 3253.9600, 3233.5300, 3253.4300],\n",
            "         [3249.1900, 3251.6500, 3224.0900, 3228.6600]],\n",
            "\n",
            "        [[3054.1100, 3090.4900, 3051.8700, 3083.5100],\n",
            "         [3085.9300, 3098.9100, 3085.9300, 3090.2300],\n",
            "         [3082.8700, 3113.5100, 3060.5300, 3112.9600],\n",
            "         [3107.8000, 3119.5800, 3101.3000, 3104.4400],\n",
            "         [3082.3300, 3103.4400, 3077.9600, 3090.1400]]], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNsItNFv3bnX"
      },
      "source": [
        "## LSTM Model\n",
        "\n",
        "**Note:**\n",
        "* `batch_first` – If `True`, then the input and output tensors are provided as `(batch, seq, feature)` instead of `(seq, batch, feature)`. \n",
        "\n",
        "- from [LSTM Documentation](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87B043P13dHK"
      },
      "source": [
        "# hyper-parameters\n",
        "input_size = 4\n",
        "hidden_size = 32 # memory_cell\n",
        "num_layers = 1\n",
        "num_class = 1 # output_layer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvMXWwhr3iAV"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, num_class):\n",
        "    super(LSTM, self).__init__()\n",
        "    \n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.normal = nn.BatchNorm1d(5, affine=True) \n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) # batch_first batch 為第一個維度\n",
        "    self.fc = nn.Linear(hidden_size, num_class)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # layer num, batch size, hidden size\n",
        "\n",
        "    h0 = torch.zeros(num_layers, x.size(0), hidden_size)    \n",
        "    c0 = torch.zeros(num_layers, x.size(0), hidden_size)\n",
        "    out = self.normal(x)\n",
        "\n",
        "    #lstm output will be output and c_n's hidden state, cell state\n",
        "    # in pytorch output is a list of each cell's hidden state\n",
        "    out, (h_out, c) = self.lstm(out, (h0, c0))\n",
        "\n",
        "    out = self.fc(out)\n",
        "    out = self.sigmoid(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOWam5y_3qy2",
        "outputId": "7fcfa80a-3088-44b4-fcbb-d8ca05570b59"
      },
      "source": [
        "model = LSTM(input_size, hidden_size, num_layers, num_class)\n",
        "model"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (normal): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (lstm): LSTM(4, 32, batch_first=True)\n",
              "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isxuUNFu8rY8",
        "outputId": "bf1bc760-eb12-4023-f056-0b6d2132e306"
      },
      "source": [
        "# fed stock data to our model \n",
        "for idx, (seqs, labels) in enumerate(train_loader):\n",
        "    input = seqs.float()\n",
        "    output = model(input)\n",
        "    print('Input Data Shape:', seqs.shape)\n",
        "    print('Output Shape:', output.size())\n",
        "    break"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Data Shape: torch.Size([4, 5, 4])\n",
            "Output Shape: torch.Size([4, 5, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSKJny0j4Pft"
      },
      "source": [
        "## Loss Function & Optimizer\n",
        "* Binary Cross Entropy Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeT_ScwH3_3_"
      },
      "source": [
        "loss_func = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfZfndhx4XNs"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_63sNuOl4MVe",
        "outputId": "a75137b1-e973-4722-ad73-c99e4101599e"
      },
      "source": [
        "for epoch in range(2000):\n",
        "  for idx, (seqs, labels) in enumerate(train_loader):\n",
        "    seqs = seqs.float()\n",
        "    labels = labels.float()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(seqs)\n",
        "    loss = loss_func(outputs, labels) # crossentropy need float number\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0:\n",
        "      print('Epoch: %d, batch: %d, Loss: %.4f'%(epoch+1, idx+1, loss.data))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100, batch: 1, Loss: 0.6648\n",
            "Epoch: 100, batch: 2, Loss: 0.6972\n",
            "Epoch: 100, batch: 3, Loss: 0.6740\n",
            "Epoch: 100, batch: 4, Loss: 0.6549\n",
            "Epoch: 100, batch: 5, Loss: 0.6607\n",
            "Epoch: 100, batch: 6, Loss: 0.6804\n",
            "Epoch: 100, batch: 7, Loss: 0.7018\n",
            "Epoch: 200, batch: 1, Loss: 0.7036\n",
            "Epoch: 200, batch: 2, Loss: 0.6180\n",
            "Epoch: 200, batch: 3, Loss: 0.5833\n",
            "Epoch: 200, batch: 4, Loss: 0.7060\n",
            "Epoch: 200, batch: 5, Loss: 0.6684\n",
            "Epoch: 200, batch: 6, Loss: 0.7125\n",
            "Epoch: 200, batch: 7, Loss: 0.6062\n",
            "Epoch: 300, batch: 1, Loss: 0.6293\n",
            "Epoch: 300, batch: 2, Loss: 0.6997\n",
            "Epoch: 300, batch: 3, Loss: 0.7605\n",
            "Epoch: 300, batch: 4, Loss: 0.6984\n",
            "Epoch: 300, batch: 5, Loss: 0.6318\n",
            "Epoch: 300, batch: 6, Loss: 0.6493\n",
            "Epoch: 300, batch: 7, Loss: 0.6824\n",
            "Epoch: 400, batch: 1, Loss: 0.6161\n",
            "Epoch: 400, batch: 2, Loss: 0.6572\n",
            "Epoch: 400, batch: 3, Loss: 0.7039\n",
            "Epoch: 400, batch: 4, Loss: 0.6604\n",
            "Epoch: 400, batch: 5, Loss: 0.6995\n",
            "Epoch: 400, batch: 6, Loss: 0.5955\n",
            "Epoch: 400, batch: 7, Loss: 0.7982\n",
            "Epoch: 500, batch: 1, Loss: 0.6598\n",
            "Epoch: 500, batch: 2, Loss: 0.6363\n",
            "Epoch: 500, batch: 3, Loss: 0.6525\n",
            "Epoch: 500, batch: 4, Loss: 0.6688\n",
            "Epoch: 500, batch: 5, Loss: 0.7297\n",
            "Epoch: 500, batch: 6, Loss: 0.6947\n",
            "Epoch: 500, batch: 7, Loss: 0.6269\n",
            "Epoch: 600, batch: 1, Loss: 0.6342\n",
            "Epoch: 600, batch: 2, Loss: 0.6515\n",
            "Epoch: 600, batch: 3, Loss: 0.6648\n",
            "Epoch: 600, batch: 4, Loss: 0.6780\n",
            "Epoch: 600, batch: 5, Loss: 0.5585\n",
            "Epoch: 600, batch: 6, Loss: 0.7835\n",
            "Epoch: 600, batch: 7, Loss: 0.6328\n",
            "Epoch: 700, batch: 1, Loss: 0.5898\n",
            "Epoch: 700, batch: 2, Loss: 0.7018\n",
            "Epoch: 700, batch: 3, Loss: 0.6427\n",
            "Epoch: 700, batch: 4, Loss: 0.7039\n",
            "Epoch: 700, batch: 5, Loss: 0.7188\n",
            "Epoch: 700, batch: 6, Loss: 0.6591\n",
            "Epoch: 700, batch: 7, Loss: 0.7290\n",
            "Epoch: 800, batch: 1, Loss: 0.5723\n",
            "Epoch: 800, batch: 2, Loss: 0.6790\n",
            "Epoch: 800, batch: 3, Loss: 0.6675\n",
            "Epoch: 800, batch: 4, Loss: 0.6504\n",
            "Epoch: 800, batch: 5, Loss: 0.7103\n",
            "Epoch: 800, batch: 6, Loss: 0.6402\n",
            "Epoch: 800, batch: 7, Loss: 0.6849\n",
            "Epoch: 900, batch: 1, Loss: 0.6431\n",
            "Epoch: 900, batch: 2, Loss: 0.6370\n",
            "Epoch: 900, batch: 3, Loss: 0.6871\n",
            "Epoch: 900, batch: 4, Loss: 0.6528\n",
            "Epoch: 900, batch: 5, Loss: 0.6726\n",
            "Epoch: 900, batch: 6, Loss: 0.5790\n",
            "Epoch: 900, batch: 7, Loss: 0.6691\n",
            "Epoch: 1000, batch: 1, Loss: 0.6137\n",
            "Epoch: 1000, batch: 2, Loss: 0.6680\n",
            "Epoch: 1000, batch: 3, Loss: 0.5593\n",
            "Epoch: 1000, batch: 4, Loss: 0.6510\n",
            "Epoch: 1000, batch: 5, Loss: 0.7737\n",
            "Epoch: 1000, batch: 6, Loss: 0.6458\n",
            "Epoch: 1000, batch: 7, Loss: 0.6243\n",
            "Epoch: 1100, batch: 1, Loss: 0.7466\n",
            "Epoch: 1100, batch: 2, Loss: 0.6106\n",
            "Epoch: 1100, batch: 3, Loss: 0.6822\n",
            "Epoch: 1100, batch: 4, Loss: 0.6129\n",
            "Epoch: 1100, batch: 5, Loss: 0.6434\n",
            "Epoch: 1100, batch: 6, Loss: 0.6492\n",
            "Epoch: 1100, batch: 7, Loss: 0.5843\n",
            "Epoch: 1200, batch: 1, Loss: 0.6767\n",
            "Epoch: 1200, batch: 2, Loss: 0.6268\n",
            "Epoch: 1200, batch: 3, Loss: 0.6155\n",
            "Epoch: 1200, batch: 4, Loss: 0.6251\n",
            "Epoch: 1200, batch: 5, Loss: 0.7057\n",
            "Epoch: 1200, batch: 6, Loss: 0.6998\n",
            "Epoch: 1200, batch: 7, Loss: 0.5152\n",
            "Epoch: 1300, batch: 1, Loss: 0.5864\n",
            "Epoch: 1300, batch: 2, Loss: 0.6313\n",
            "Epoch: 1300, batch: 3, Loss: 0.6661\n",
            "Epoch: 1300, batch: 4, Loss: 0.6618\n",
            "Epoch: 1300, batch: 5, Loss: 0.6103\n",
            "Epoch: 1300, batch: 6, Loss: 0.7110\n",
            "Epoch: 1300, batch: 7, Loss: 0.6878\n",
            "Epoch: 1400, batch: 1, Loss: 0.7223\n",
            "Epoch: 1400, batch: 2, Loss: 0.7121\n",
            "Epoch: 1400, batch: 3, Loss: 0.5756\n",
            "Epoch: 1400, batch: 4, Loss: 0.7165\n",
            "Epoch: 1400, batch: 5, Loss: 0.5367\n",
            "Epoch: 1400, batch: 6, Loss: 0.5860\n",
            "Epoch: 1400, batch: 7, Loss: 0.8129\n",
            "Epoch: 1500, batch: 1, Loss: 0.6537\n",
            "Epoch: 1500, batch: 2, Loss: 0.6951\n",
            "Epoch: 1500, batch: 3, Loss: 0.5872\n",
            "Epoch: 1500, batch: 4, Loss: 0.5786\n",
            "Epoch: 1500, batch: 5, Loss: 0.6316\n",
            "Epoch: 1500, batch: 6, Loss: 0.6865\n",
            "Epoch: 1500, batch: 7, Loss: 0.6942\n",
            "Epoch: 1600, batch: 1, Loss: 0.6258\n",
            "Epoch: 1600, batch: 2, Loss: 0.6796\n",
            "Epoch: 1600, batch: 3, Loss: 0.6181\n",
            "Epoch: 1600, batch: 4, Loss: 0.6420\n",
            "Epoch: 1600, batch: 5, Loss: 0.6444\n",
            "Epoch: 1600, batch: 6, Loss: 0.6446\n",
            "Epoch: 1600, batch: 7, Loss: 0.7855\n",
            "Epoch: 1700, batch: 1, Loss: 0.6748\n",
            "Epoch: 1700, batch: 2, Loss: 0.5899\n",
            "Epoch: 1700, batch: 3, Loss: 0.6696\n",
            "Epoch: 1700, batch: 4, Loss: 0.5760\n",
            "Epoch: 1700, batch: 5, Loss: 0.7150\n",
            "Epoch: 1700, batch: 6, Loss: 0.7159\n",
            "Epoch: 1700, batch: 7, Loss: 0.4434\n",
            "Epoch: 1800, batch: 1, Loss: 0.6575\n",
            "Epoch: 1800, batch: 2, Loss: 0.6403\n",
            "Epoch: 1800, batch: 3, Loss: 0.6167\n",
            "Epoch: 1800, batch: 4, Loss: 0.6192\n",
            "Epoch: 1800, batch: 5, Loss: 0.6919\n",
            "Epoch: 1800, batch: 6, Loss: 0.6358\n",
            "Epoch: 1800, batch: 7, Loss: 0.8124\n",
            "Epoch: 1900, batch: 1, Loss: 0.6916\n",
            "Epoch: 1900, batch: 2, Loss: 0.6452\n",
            "Epoch: 1900, batch: 3, Loss: 0.6376\n",
            "Epoch: 1900, batch: 4, Loss: 0.6582\n",
            "Epoch: 1900, batch: 5, Loss: 0.5874\n",
            "Epoch: 1900, batch: 6, Loss: 0.6907\n",
            "Epoch: 1900, batch: 7, Loss: 0.6382\n",
            "Epoch: 2000, batch: 1, Loss: 0.4968\n",
            "Epoch: 2000, batch: 2, Loss: 0.6354\n",
            "Epoch: 2000, batch: 3, Loss: 0.6452\n",
            "Epoch: 2000, batch: 4, Loss: 0.6670\n",
            "Epoch: 2000, batch: 5, Loss: 0.7377\n",
            "Epoch: 2000, batch: 6, Loss: 0.7109\n",
            "Epoch: 2000, batch: 7, Loss: 0.5550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQKOdGzl-MbB"
      },
      "source": [
        "## Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQZgMhMP4c5P",
        "outputId": "020c3793-dbdf-49d9-f25f-1cf6641b6372"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "for seqs, labels in test_loader:\n",
        "  seqs = seqs.float()\n",
        "  outputs = model(seqs).float()\n",
        "\n",
        "  predicted = outputs.gt(0.5) # bool\n",
        "  total += (labels.size(0) * labels.size(1))\n",
        "  correct += (predicted == labels).sum()\n",
        "\n",
        "print('Acc: %.3f %%' % (100.0 * float(correct)//float(total)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc: 61.000 %\n"
          ]
        }
      ]
    }
  ]
}