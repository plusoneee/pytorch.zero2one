{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyE-FvU1oOGF"
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader as DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as opt\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8UtmGYDoRud"
      },
      "source": [
        "def to_var(x):\n",
        "  if torch.cuda.is_available():\n",
        "    x = x.cuda()\n",
        "  return Variable(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTQPzLopoSFs"
      },
      "source": [
        "# 超參數\n",
        "num_epoch = 30\n",
        "d_learning_rate = 0.001\n",
        "g_learning_rate = 0.0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIHFwJ4DoTbM"
      },
      "source": [
        "# define tansform\n",
        "transform = transforms.Compose([\n",
        "  transforms.ToTensor(),\n",
        "  transforms.Normalize(mean=0.5, std=0.5) # std 標準差\n",
        "]) # Compose input type: list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81kagAGroUZs"
      },
      "source": [
        "train_dataset = dsets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yv-3whyo0N8"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, latent_dim=100, batch_norm=True):\n",
        "    super(Generator, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.batch_norm = batch_norm\n",
        "    self.linear_1 = nn.Linear(self.latent_dim, 256*7*7, bias=False) # 256channels\n",
        "    self.batch_norm_1 = nn.BatchNorm1d(256*7*7) if batch_norm else None\n",
        "    self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "    \n",
        "    # ConvTranspose2d 放大圖片(DE-CONV)\n",
        "    self.conv_1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=5, stride=1, padding=2, bias=False)\n",
        "    self.batch_norm_2d_1 = nn.BatchNorm2d(128) if batch_norm else None\n",
        "    self.conv_2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1, bias=False)\n",
        "    self.batch_norm_2d_2 = nn.BatchNorm2d(64) if batch_norm else None\n",
        "    self.conv_3 = nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=4, stride=2, padding=1, bias=False) # output\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.linear_1(x)\n",
        "    if self.batch_norm:\n",
        "      out = self.batch_norm_1(out)\n",
        "    out = self.leaky_relu(out)\n",
        "    # 1D to 2D\n",
        "    out = out.view(-1, 256, 7, 7) # batch_size, channels, row, col\n",
        "    out = self.conv_1(out)\n",
        "    if self.batch_norm:\n",
        "      out = self.batch_norm_2d_1(out)\n",
        "    out = self.conv_2(out)\n",
        "    if self.batch_norm:\n",
        "      out = self.batch_norm_2d_2(out)\n",
        "\n",
        "    out = self.conv_3(out)\n",
        "    out = self.tanh(out)\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_hcJdGBvVwQ"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.conv_1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=5, stride=2, padding=2, bias=True)\n",
        "    self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "    self.dropout_2d = nn.Dropout2d(p=0.3)\n",
        "    self.conv_2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=5, stride=2, padding=2, bias=True)\n",
        "    self.linear_1 = nn.Linear(128*7*7, 1, bias=True)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv_1(x)\n",
        "    out = self.leaky_relu(out)\n",
        "    out = self.dropout_2d(out)\n",
        "    out = self.conv_2(out)\n",
        "    # 2D to 1D\n",
        "    out = out.view(-1, 128*7*7)\n",
        "    out = self.linear_1(out)\n",
        "    out = self.sigmoid(out)\n",
        "\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKZ7-pguogKT"
      },
      "source": [
        "G = Generator()\n",
        "D = Discriminator()\n",
        "# GPU|\n",
        "if torch.cuda.is_available():\n",
        "  D.cuda()\n",
        "  G.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vf0XfY429WQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "6f50a507-5fce-4df9-beb0-8a6a0b1033bf"
      },
      "source": [
        "print(D)\n",
        "print(G)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator(\n",
            "  (conv_1): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
            "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
            "  (dropout_2d): Dropout2d(p=0.3, inplace=False)\n",
            "  (conv_2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
            "  (linear_1): Linear(in_features=6272, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n",
            "Generator(\n",
            "  (linear_1): Linear(in_features=100, out_features=12544, bias=False)\n",
            "  (batch_norm_1): BatchNorm1d(12544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
            "  (conv_1): ConvTranspose2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "  (batch_norm_2d_1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (batch_norm_2d_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv_3): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (tanh): Tanh()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KUxjZ48ohQU"
      },
      "source": [
        "loss_func = nn.BCELoss() # binary cross entropy\n",
        "d_opt = opt.Adam(D.parameters(), lr=0.001, betas=(0.5, 0.999)) #betas (beta1, beta2) \n",
        "g_opt = opt.Adam(G.parameters(), lr=0.0001, betas=(0.5, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM_nzgiJoiNj"
      },
      "source": [
        "def de_normalize(x):\n",
        "  out = (x + 1)/2\n",
        "  return out.clamp(0, 1) # 所有小於0的都等於0, 所有大於1的都等於1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS3y99iKojKU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "121abd04-505c-4840-aa9d-55df1abeb380"
      },
      "source": [
        "for epoch in range(num_epoch):\n",
        "  for i, (images, _) in enumerate(train_loader):\n",
        "    batch_size = images.size(0) # images.size(0) 總共多少資料\n",
        "    images = to_var(images)\n",
        "\n",
        "    # 真實圖片的 LABEL 都為1\n",
        "    real_labels = to_var(torch.ones(batch_size, 1)) # (row, col)\n",
        "    # 假圖片的 LABEL 都為0\n",
        "    fake_labels = to_var(torch.zeros(batch_size, 1)) # (row, col)\n",
        "\n",
        "    outputs = D(images)\n",
        "    d_loss_at_real = loss_func(outputs, real_labels)\n",
        "    real_score = outputs\n",
        "\n",
        "    z = to_var(torch.randn(batch_size, 100)) # 要餵給G的隨機產生的向量, 100為G的輸入向量\n",
        "    fake_images = G(z) \n",
        "    outputs = D(fake_images) # or output = D(G(z))\n",
        "    \n",
        "    d_loss_at_fake = loss_func(outputs, fake_labels) # output和0的距離\n",
        "    fake_score = outputs # fake 越高表示G騙過D\n",
        "\n",
        "    d_loss = d_loss_at_real + d_loss_at_fake # total loss\n",
        "    D.zero_grad()\n",
        "    d_loss.backward()\n",
        "    d_opt.step()\n",
        "\n",
        "    # 需要重新產生fake image\n",
        "    z = to_var(torch.randn(batch_size, 100)) # 要餵給G的隨機產生的向量, 64為G的輸入向量\n",
        "    fake_images = G(z) \n",
        "    outputs = D(fake_images) # or output = D(G(z))\n",
        "\n",
        "    g_loss = loss_func(outputs, real_labels) # output和1的距離\n",
        "    D.zero_grad() # because D(G(z))\n",
        "    G.zero_grad()\n",
        "    g_loss.backward()\n",
        "    g_opt.step()\n",
        "\n",
        "    if (i+1)%300 == 0:\n",
        "      print('Epoch[%d], Batch[%d], d_loss:%.4f, g_loss:%.4f, D(x): %.2f, D(G(x)): %.2f'%(epoch+1, i+1, d_loss.data, g_loss.data, real_score.mean(), fake_score.mean()))\n",
        "  \n",
        "  if (epoch == 0): # 第一次先存正常的圖\n",
        "    images = images.view(images.size(0), 1, 28, 28)\n",
        "    save_image(de_normalize(images.data), './data/real_images.png')\n",
        "\n",
        "  fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
        "  save_image(de_normalize(fake_images.data), './data/fake_images_'+str(epoch+1)+'.png')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch[1], Batch[300], d_loss:0.0000, g_loss:50.7319, D(x): 1.00, D(G(x)): 0.00\n",
            "Epoch[1], Batch[600], d_loss:0.0000, g_loss:47.4435, D(x): 1.00, D(G(x)): 0.00\n",
            "Epoch[2], Batch[300], d_loss:1.0271, g_loss:1.7022, D(x): 0.61, D(G(x)): 0.29\n",
            "Epoch[2], Batch[600], d_loss:1.2399, g_loss:0.9792, D(x): 0.48, D(G(x)): 0.28\n",
            "Epoch[3], Batch[300], d_loss:0.9877, g_loss:1.7790, D(x): 0.72, D(G(x)): 0.44\n",
            "Epoch[3], Batch[600], d_loss:1.0953, g_loss:1.1450, D(x): 0.53, D(G(x)): 0.25\n",
            "Epoch[4], Batch[300], d_loss:1.0050, g_loss:1.5582, D(x): 0.65, D(G(x)): 0.33\n",
            "Epoch[4], Batch[600], d_loss:0.7049, g_loss:1.3121, D(x): 0.69, D(G(x)): 0.19\n",
            "Epoch[5], Batch[300], d_loss:0.8731, g_loss:3.4256, D(x): 0.87, D(G(x)): 0.41\n",
            "Epoch[5], Batch[600], d_loss:0.6345, g_loss:2.5783, D(x): 0.76, D(G(x)): 0.15\n",
            "Epoch[6], Batch[300], d_loss:0.5951, g_loss:3.0970, D(x): 0.84, D(G(x)): 0.24\n",
            "Epoch[6], Batch[600], d_loss:0.5301, g_loss:2.7596, D(x): 0.86, D(G(x)): 0.19\n",
            "Epoch[7], Batch[300], d_loss:0.4147, g_loss:2.4727, D(x): 0.87, D(G(x)): 0.16\n",
            "Epoch[7], Batch[600], d_loss:0.6282, g_loss:2.9064, D(x): 0.85, D(G(x)): 0.23\n",
            "Epoch[8], Batch[300], d_loss:1.6201, g_loss:6.8477, D(x): 0.93, D(G(x)): 0.52\n",
            "Epoch[8], Batch[600], d_loss:0.4751, g_loss:2.6656, D(x): 0.91, D(G(x)): 0.21\n",
            "Epoch[9], Batch[300], d_loss:0.6455, g_loss:1.7964, D(x): 0.70, D(G(x)): 0.04\n",
            "Epoch[9], Batch[600], d_loss:0.5128, g_loss:1.9171, D(x): 0.81, D(G(x)): 0.11\n",
            "Epoch[10], Batch[300], d_loss:0.3568, g_loss:2.7554, D(x): 0.85, D(G(x)): 0.09\n",
            "Epoch[10], Batch[600], d_loss:0.4657, g_loss:2.5932, D(x): 0.85, D(G(x)): 0.13\n",
            "Epoch[11], Batch[300], d_loss:0.4938, g_loss:4.0121, D(x): 0.90, D(G(x)): 0.19\n",
            "Epoch[11], Batch[600], d_loss:0.5668, g_loss:3.3614, D(x): 0.84, D(G(x)): 0.16\n",
            "Epoch[12], Batch[300], d_loss:0.4084, g_loss:3.2349, D(x): 0.87, D(G(x)): 0.13\n",
            "Epoch[12], Batch[600], d_loss:0.5253, g_loss:2.6230, D(x): 0.85, D(G(x)): 0.18\n",
            "Epoch[13], Batch[300], d_loss:0.4217, g_loss:2.7170, D(x): 0.81, D(G(x)): 0.06\n",
            "Epoch[13], Batch[600], d_loss:0.3549, g_loss:3.5850, D(x): 0.89, D(G(x)): 0.13\n",
            "Epoch[14], Batch[300], d_loss:0.5889, g_loss:2.4568, D(x): 0.80, D(G(x)): 0.09\n",
            "Epoch[14], Batch[600], d_loss:0.6136, g_loss:4.2411, D(x): 0.88, D(G(x)): 0.23\n",
            "Epoch[15], Batch[300], d_loss:0.6275, g_loss:3.8700, D(x): 0.92, D(G(x)): 0.31\n",
            "Epoch[15], Batch[600], d_loss:0.5903, g_loss:2.1953, D(x): 0.82, D(G(x)): 0.19\n",
            "Epoch[16], Batch[300], d_loss:0.8905, g_loss:1.6401, D(x): 0.65, D(G(x)): 0.04\n",
            "Epoch[16], Batch[600], d_loss:0.5632, g_loss:2.5994, D(x): 0.79, D(G(x)): 0.10\n",
            "Epoch[17], Batch[300], d_loss:0.5285, g_loss:3.0110, D(x): 0.85, D(G(x)): 0.18\n",
            "Epoch[17], Batch[600], d_loss:0.4998, g_loss:3.1434, D(x): 0.83, D(G(x)): 0.14\n",
            "Epoch[18], Batch[300], d_loss:0.6684, g_loss:2.9832, D(x): 0.82, D(G(x)): 0.21\n",
            "Epoch[18], Batch[600], d_loss:0.6058, g_loss:2.7536, D(x): 0.76, D(G(x)): 0.08\n",
            "Epoch[19], Batch[300], d_loss:0.6330, g_loss:3.0762, D(x): 0.76, D(G(x)): 0.10\n",
            "Epoch[19], Batch[600], d_loss:0.5398, g_loss:2.3814, D(x): 0.85, D(G(x)): 0.19\n",
            "Epoch[20], Batch[300], d_loss:0.5649, g_loss:2.5663, D(x): 0.85, D(G(x)): 0.23\n",
            "Epoch[20], Batch[600], d_loss:0.5994, g_loss:2.5809, D(x): 0.82, D(G(x)): 0.22\n",
            "Epoch[21], Batch[300], d_loss:0.5556, g_loss:2.4098, D(x): 0.84, D(G(x)): 0.18\n",
            "Epoch[21], Batch[600], d_loss:0.5974, g_loss:2.8429, D(x): 0.79, D(G(x)): 0.16\n",
            "Epoch[22], Batch[300], d_loss:0.5545, g_loss:2.1178, D(x): 0.78, D(G(x)): 0.17\n",
            "Epoch[22], Batch[600], d_loss:0.5927, g_loss:2.8952, D(x): 0.77, D(G(x)): 0.07\n",
            "Epoch[23], Batch[300], d_loss:0.6314, g_loss:2.1208, D(x): 0.76, D(G(x)): 0.11\n",
            "Epoch[23], Batch[600], d_loss:0.6929, g_loss:3.0999, D(x): 0.86, D(G(x)): 0.28\n",
            "Epoch[24], Batch[300], d_loss:0.5833, g_loss:2.7072, D(x): 0.85, D(G(x)): 0.22\n",
            "Epoch[24], Batch[600], d_loss:0.5738, g_loss:2.7292, D(x): 0.78, D(G(x)): 0.10\n",
            "Epoch[25], Batch[300], d_loss:0.6140, g_loss:2.6892, D(x): 0.82, D(G(x)): 0.19\n",
            "Epoch[25], Batch[600], d_loss:1.1009, g_loss:1.7583, D(x): 0.62, D(G(x)): 0.11\n",
            "Epoch[26], Batch[300], d_loss:0.8143, g_loss:3.5144, D(x): 0.85, D(G(x)): 0.31\n",
            "Epoch[26], Batch[600], d_loss:0.6508, g_loss:2.9287, D(x): 0.85, D(G(x)): 0.26\n",
            "Epoch[27], Batch[300], d_loss:0.6377, g_loss:2.1542, D(x): 0.81, D(G(x)): 0.21\n",
            "Epoch[27], Batch[600], d_loss:0.6520, g_loss:1.9577, D(x): 0.78, D(G(x)): 0.19\n",
            "Epoch[28], Batch[300], d_loss:0.6933, g_loss:2.2953, D(x): 0.78, D(G(x)): 0.19\n",
            "Epoch[28], Batch[600], d_loss:0.8032, g_loss:2.0318, D(x): 0.67, D(G(x)): 0.08\n",
            "Epoch[29], Batch[300], d_loss:0.6429, g_loss:1.7385, D(x): 0.75, D(G(x)): 0.12\n",
            "Epoch[29], Batch[600], d_loss:0.6762, g_loss:2.8553, D(x): 0.89, D(G(x)): 0.32\n",
            "Epoch[30], Batch[300], d_loss:0.7469, g_loss:2.1445, D(x): 0.72, D(G(x)): 0.15\n",
            "Epoch[30], Batch[600], d_loss:0.5966, g_loss:2.5744, D(x): 0.80, D(G(x)): 0.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNcp82G02fVY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}