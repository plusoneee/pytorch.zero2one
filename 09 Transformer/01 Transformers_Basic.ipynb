{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformers Basic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKYT_MiG1Ukk"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cid8bIAIm4x",
        "outputId": "0622aad4-aff9-49a5-8879-e16c6229655d"
      },
      "source": [
        "#  X with size (b, t, k)\n",
        "# A sequence of t vectors of dimension k as a t by k matrix ùêó\n",
        "x = torch.randn(1, 3, 4)\n",
        "print('x shape:', x.shape)\n",
        "print('xT shape:', x.transpose(1, 2).shape)\n",
        "raw_weights = torch.bmm(x, x.transpose(1, 2))\n",
        "print('w shape:', raw_weights.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x shape: torch.Size([1, 3, 4])\n",
            "xT shape: torch.Size([1, 4, 3])\n",
            "w shape: torch.Size([1, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cxcsu9omIr4c",
        "outputId": "6be9dc66-8990-424a-9d9b-c63c8721651a"
      },
      "source": [
        "# To turn the raw weights w‚Ä≤ij into positive values that sum to one\n",
        "# apply a row-wise softmax\n",
        "print(raw_weights)\n",
        "weights = F.softmax(raw_weights, dim=2)\n",
        "weights"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 1.3397,  0.3852, -0.1114],\n",
            "         [ 0.3852,  5.6949,  0.8862],\n",
            "         [-0.1114,  0.8862,  3.1280]]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.6176, 0.2377, 0.1447],\n",
              "         [0.0049, 0.9871, 0.0081],\n",
              "         [0.0342, 0.0928, 0.8730]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOs2uL8aI9qO",
        "outputId": "87c78026-fe29-4c1d-9daf-9b02aff7a05d"
      },
      "source": [
        "# To compute the output sequence, we just multiply the weight matrix by ùêó. \n",
        "# This results in a batch of output matrices ùêò of size (b, t, k) whose rows are weighted sums over the rows of ùêó.\n",
        "# That‚Äôs all. Two matrix multiplications and one softmax gives us a basic self-attention.\n",
        "\n",
        "y = torch.bmm(weights, x) # (1, 3, 3) x (1, 3, 4) = (1, 3, 4) \n",
        "y.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bprOIp7LJb9Z"
      },
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, emb, heads=8):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.emb = emb\n",
        "        self.heads = heads\n",
        "        # These compute the queries, keys and values for all \n",
        "        # heads (as a single concatenated vector)\n",
        "        self.tokeys    = nn.Linear(emb, emb * heads, bias=False)\n",
        "        self.toqueries = nn.Linear(emb, emb * heads, bias=False)\n",
        "        self.tovalues  = nn.Linear(emb, emb * heads, bias=False)\n",
        "\n",
        "        # This unifies the outputs of the different heads into a single k-vector\n",
        "        self.unifyheads = nn.Linear(heads * emb, emb)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        b, t, e = x.size()\n",
        "        h = self.heads\n",
        "\n",
        "        assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
        "\n",
        "        queries = self.toqueries(x).view(b, t, h, e)\n",
        "        keys = self.tokeys(x).view(b, t, h, e)\n",
        "        values = self.tovalues(x).view(b, t, h, e)\n",
        "\n",
        "        # - fold heads into the batch dimension\n",
        "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, e)\n",
        "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, e)\n",
        "        values = values.transpose(1, 2).contiguous().view(b * h, t, e)\n",
        "\n",
        "        queries = queries / (k ** (1/4))\n",
        "        keys    = keys / (k ** (1/4))\n",
        "\n",
        "        # - get dot product of queries and keys, and scale\n",
        "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
        "        # - dot has size (b*h, t, t) containing raw weights\n",
        "\n",
        "        dot = F.softmax(dot, dim=2) \n",
        "        # - dot now contains row-wise normalized weights\n",
        "\n",
        "        # apply the self attention to the values\n",
        "        out = torch.bmm(dot, values).view(b, h, t, e)\n",
        "\n",
        "        # swap h, t back, unify heads\n",
        "        out = out.transpose(1, 2).contiguous().view(b, t, h * e)\n",
        "        return self.unifyheads(out)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLk8iprlKEjM"
      },
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, emb, heads, ff_hidden_mult=4, dropout=0.0):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        \n",
        "        self.attention = SelfAttention(emb, heads=heads)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(emb)\n",
        "        slef.norm2 = nn.LayerNorm(emb)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "   \n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(emb, ff_hidden_mult*emb),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_hidden_mult*emb, emb)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attented = self.attention(x)\n",
        "\n",
        "        out = self.norm1(attended + x)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        fedforward = self.ff(out)\n",
        "\n",
        "        out = self.norm2(ffedforward + out)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpW-gUJxbPJY"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0):\n",
        "        \"\"\"\n",
        "        :param emb: Embedding dimension\n",
        "        :param heads: nr. of attention heads\n",
        "        :param depth: Number of transformer blocks\n",
        "        :param seq_length: Expected maximum sequence length\n",
        "        :param num_tokens: Number of tokens (usually words) in the vocabulary\n",
        "        :param num_classes: Number of classes.\n",
        "        :param max_pool: If true, use global max pooling in the last layer. If false, use global\n",
        "                         average pooling.\n",
        "        \"\"\"\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.max_poo = max_pool\n",
        "        self.num_tokens = num_tokens\n",
        "\n",
        "        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
        "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
        "        \n",
        "\n",
        "        self.unify_embeddings = nn.Linear(2 * emb, emb)\n",
        "\n",
        "        tblocks = []\n",
        "        for i in range(depth):\n",
        "            tblocks.append(TransformerBlock(emb, heads))\n",
        "        \n",
        "        self.tblocks = nn.Sequential(*tblocks)\n",
        "        # Maps the final output sequence to class logits\n",
        "        self.toprobs = nn.Linear(k, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        :param x: A batch by sequence length integer tensor of token indices.\n",
        "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        # generate token embeddings\n",
        "        tokens = self.token_embedding(x)\n",
        "        b, t, e = tokens.size()\n",
        "\n",
        "        # generate position embeddings\n",
        "        positions = torch.arange(t)\n",
        "        positions = self.pos_emb(positions)[None, :, :].expand(b, t, e)\n",
        "\n",
        "        out = torch.cat((tokens, positions), dim=2).view(-1, 2*e)\n",
        "        out = self.unify_embeddings(out).view(b, t, e)\n",
        "\n",
        "        out = self.dropout(out)\n",
        "        out = self.tblocks(out)\n",
        "\n",
        "        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1) # pool over the time dimension\n",
        "        \n",
        "        # probabilities\n",
        "        x = self.toprobs(out)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}